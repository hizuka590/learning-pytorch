{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80eb7bde",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.d_model = 20\n",
    "        self.n_heads = 2\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        dim_k  = d_model % n_heads\n",
    "        dim_v = d_model % n_heads\n",
    "\n",
    "        self.padding_size = 30\n",
    "        self.UNK = 5\n",
    "        self.PAD = 4\n",
    "\n",
    "        self.N = 6\n",
    "        self.p = 0.1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        # 一个普通的 embedding层，我们可以通过设置padding_idx=config.PAD 来实现论文中的 padding_mask\n",
    "        self.embedding = nn.Embedding(vocab_size,config.d_model,padding_idx=config.PAD)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 根据每个句子的长度，进行padding，短补长截\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i]) < config.padding_size:\n",
    "                x[i].extend([config.UNK] * (config.padding_size - len(x[i]))) # 注意 UNK是你词表中用来表示oov的token索引，这里进行了简化，直接假设为6\n",
    "            else:\n",
    "                x[i] = x[i][:config.padding_size]\n",
    "        x = self.embedding(torch.tensor(x)) # batch_size * seq_len * d_model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model):\n",
    "        super(Positional_Encoding,self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "    def forward(self,seq_len,embedding_dim):\n",
    "        positional_encoding = np.zeros((seq_len,embedding_dim))\n",
    "        for pos in range(positional_encoding.shape[0]):\n",
    "            for i in range(positional_encoding.shape[1]):\n",
    "                positional_encoding[pos][i] = math.sin(pos/(10000**(2*i/self.d_model))) if i % 2 == 0 else math.cos(pos/(10000**(2*i/self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
